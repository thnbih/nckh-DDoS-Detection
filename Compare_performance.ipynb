import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score
import matplotlib.pyplot as plt
from google.colab import drive
drive.mount('/content/drive')

Path = '/content/drive/MyDrive/dataset'

"""#Load data file UDP, UDPLag, Portmap, Syn


"""

# csv_file = Path + '/Syn.csv'
# chunksize = 10000
# chunks = []
# for chunk in pd.read_csv(csv_file, chunksize=chunksize):
#         chunks.append(chunk)
# data_syn = pd.concat(chunks)
# del chunks

csv_file_udp = Path + '/UDP.csv'
chunksize = 10000
chunks = []
for chunk in pd.read_csv(csv_file_udp, chunksize=chunksize):
    chunks.append(chunk)
data_udp = pd.concat(chunks)
del chunks

# List of different dataset paths
data_udplag = pd.read_csv(Path + '/UDPLag.csv')
data_portmap = pd.read_csv(Path + '/Portmap.csv')

dataset_paths = [data_udp, data_udplag, data_portmap]

"""Lists to store performance results"""

accuracies = []
f1_scores = []

"""# Feature extraction"""

import numpy as np
for data in dataset_paths:

    # Select features and labels
    X = data[[' Total Fwd Packets', 'Total Length of Fwd Packets', ' Flow Duration', ' Flow Packets/s']]
    y = data[' Label']

    # Split into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # print(np.isinf(X_train).sum())
    # print(np.max(X_train))
    # print(np.isinf(X_test).sum())
    # print(np.max(X_test))

    X_train.replace([np.inf, -np.inf], np.nan, inplace=True)
    X_train.fillna(X_train.mean(), inplace=True)
    X_test.replace([np.inf, -np.inf], np.nan, inplace=True)
    X_test.fillna(X_test.mean(), inplace=True)

    # Normalize data
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # PCA for feature reduction
    pca = PCA(n_components=4)
    X_train_pca = pca.fit_transform(X_train)
    X_test_pca = pca.transform(X_test)

    # Create a Random Forest classifier
    rf = RandomForestClassifier(n_estimators=30, random_state=42)

    # Train the classifier
    rf.fit(X_train_pca, y_train)

    # Predict on the test data
    y_pred = rf.predict(X_test_pca)

    # Calculate accuracy and F1-score
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='weighted')

    # Store the results
    accuracies.append(accuracy)
    f1_scores.append(f1)

"""# Print results for each dataset"""

# Extract dataset names
dataset_names = ['UDP','UDPLag', 'Portmap']

for i, dataset in enumerate(dataset_paths):
    dataset_name = dataset_names[i]
    print(f"Dataset: {dataset_name}")
    print(f"Accuracy: {accuracies[i]}, F1-score: {f1_scores[i]}")
    print()

"""# Create plots to compare performance"""

# Create plots to compare performance
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.bar(dataset_names, accuracies)
plt.xlabel('Dataset')
plt.ylabel('Accuracy')
plt.title('Accuracy Comparison')

plt.subplot(1, 2, 2)
plt.bar(dataset_names, f1_scores)
plt.xlabel('Dataset')
plt.ylabel('F1-score')
plt.title('F1-score Comparison')

plt.tight_layout()
plt.show()

from sklearn.metrics import precision_score, recall_score, confusion_matrix, roc_curve, auc
for i, dataset in enumerate(dataset_paths):
# Select features and labels
    X = data[[' Total Fwd Packets', 'Total Length of Fwd Packets', ' Flow Duration', ' Flow Packets/s']]
    y = data[' Label']

    # Split into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # print(np.isinf(X_train).sum())
    # print(np.max(X_train))
    # print(np.isinf(X_test).sum())
    # print(np.max(X_test))

    X_train.replace([np.inf, -np.inf], np.nan, inplace=True)
    X_train.fillna(X_train.mean(), inplace=True)
    X_test.replace([np.inf, -np.inf], np.nan, inplace=True)
    X_test.fillna(X_test.mean(), inplace=True)

    # Normalize data
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # PCA for feature reduction
    pca = PCA(n_components=4)
    X_train_pca = pca.fit_transform(X_train)
    X_test_pca = pca.transform(X_test)

    # Create a Random Forest classifier
    rf = RandomForestClassifier(n_estimators=30, random_state=42)

    # Train the classifier
    rf.fit(X_train_pca, y_train)

    # Predict on the test data
    y_pred = rf.predict(X_test_pca)

    # Calculate accuracy, F1-score, Precision, and Recall
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='weighted')
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)

    # ROC Curve and AUC
    # ROC Curve and AUC
    fpr, tpr, thresholds = roc_curve(y_test, rf.predict_proba(X_test_pca)[:,1], pos_label='Portmap')
    roc_auc = auc(fpr, tpr)

    # Print results
    print(f"Dataset: {dataset_names[i]}")
    print(f"Accuracy: {accuracy}, F1-score: {f1}")
    print(f"Precision: {precision}, Recall: {recall}")
    print(f"Confusion Matrix:\n{cm}")
    print(f"ROC AUC: {roc_auc}")
    print()
